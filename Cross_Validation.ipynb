{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO1a0OUsU2EA1uTnlu3Yp3P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsholofelo-mokheleli/ACIS-2023-New-Zealand/blob/main/Cross_Validation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Imbalance Techniques Experiment + Cross Validation**\n",
        "\n",
        "**Cross-validation**\n",
        "\n",
        "* To assess the model's performance robustly.\n",
        "* It helps you avoid overfitting\n",
        "* Provides a more reliable estimate of your model's accuracy.\n",
        "\n",
        "When performing cross-validation, the main results you obtain are the mean cross-validation accuracy and the standard deviation of cross-validation accuracy. These two metrics provide valuable information about the model's performance and its consistency across different folds.\n",
        "\n",
        "1. **Mean Cross-Validation Accuracy:** This value represents the average accuracy of the model across all the cross-validation folds. It gives you an estimate of how well the model is likely to perform on unseen data.\n",
        "\n",
        "2. **Standard Deviation of Cross-Validation Accuracy:** The standard deviation provides a measure of how much the accuracy scores vary across the different folds. A smaller standard deviation indicates that the model's performance is relatively consistent across folds, while a larger standard deviation may suggest that the model's performance is more sensitive to the particular training and validation splits.\n",
        "\n",
        "In summary, these two metrics together give you an understanding of the model's average performance and its consistency, which is important in assessing the model's generalization capabilities and making informed decisions about its deployment."
      ],
      "metadata": {
        "id": "IQ4ySBu6T_0U"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "PyWAcwC-RE75"
      },
      "outputs": [],
      "source": [
        "# Load the libraries\n",
        "\n",
        "import pandas  as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import psutil\n",
        "import os\n",
        "\n",
        "# Warning filter\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "cmap=sns.color_palette('Blues_r')\n",
        "\n",
        "# Metrics\n",
        "\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, cohen_kappa_score, confusion_matrix, balanced_accuracy_score\n",
        "from imblearn.metrics import geometric_mean_score\n",
        "\n",
        "# Preprocessing\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Algorithmns models\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Ensemble Methods\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "\n",
        "# Class imbalance\n",
        "\n",
        "from imblearn.under_sampling import TomekLinks\n",
        "from imblearn.under_sampling import NearMiss\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "from sklearn.utils import resample\n",
        "\n",
        "# Plot Theme\n",
        "\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "plt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Data**"
      ],
      "metadata": {
        "id": "p6Z9OpoxYnrz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv(\"KNN Imputation Dataset.csv\")"
      ],
      "metadata": {
        "id": "-RKEhDySYpWB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Label Encode, Drop null from target var, and Convert data to Int**"
      ],
      "metadata": {
        "id": "uNnfUsRQ22VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# make small letters\n",
        "data['country'] = data['country'].apply(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "\n",
        "# Fit and transform the countries data\n",
        "data[\"country\"] = label_encoder.fit_transform(data['country'])\n",
        "\n",
        "data = data.dropna()\n",
        "\n",
        "# Convert all columns to int data type\n",
        "for column in data.columns:\n",
        "    data[column] = data[column].astype('int64')"
      ],
      "metadata": {
        "id": "vURbuIux2buQ"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tomgt_H6YyUQ",
        "outputId": "b4dc2f77-0713-4ad2-ad28-43cbe32bb9de"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 2189 entries, 0 to 3268\n",
            "Data columns (total 25 columns):\n",
            " #   Column                    Non-Null Count  Dtype\n",
            "---  ------                    --------------  -----\n",
            " 0   self_employed             2189 non-null   int64\n",
            " 1   no_employees              2189 non-null   int64\n",
            " 2   tech_company              2189 non-null   int64\n",
            " 3   company_role              2189 non-null   int64\n",
            " 4   benefits                  2189 non-null   int64\n",
            " 5   care_options              2189 non-null   int64\n",
            " 6   wellness_program          2189 non-null   int64\n",
            " 7   seek_help                 2189 non-null   int64\n",
            " 8   anonymity                 2189 non-null   int64\n",
            " 9   leave                     2189 non-null   int64\n",
            " 10  mental_importance         2189 non-null   int64\n",
            " 11  neg_consequence_coworker  2189 non-null   int64\n",
            " 12  discuss_mh                2189 non-null   int64\n",
            " 13  work_interfere            2189 non-null   int64\n",
            " 14  coworkers                 2189 non-null   int64\n",
            " 15  supervisor                2189 non-null   int64\n",
            " 16  mental_health_interview   2189 non-null   int64\n",
            " 17  family_history            2189 non-null   int64\n",
            " 18  past_mental_health        2189 non-null   int64\n",
            " 19  mental_health             2189 non-null   int64\n",
            " 20  mental_health_diagnosed   2189 non-null   int64\n",
            " 21  treatment                 2189 non-null   int64\n",
            " 22  age                       2189 non-null   int64\n",
            " 23  gender                    2189 non-null   int64\n",
            " 24  country                   2189 non-null   int64\n",
            "dtypes: int64(25)\n",
            "memory usage: 444.6 KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Split Dataset**"
      ],
      "metadata": {
        "id": "Q8o_iTodY8-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.drop([\"mental_health_diagnosed\"], axis=1)\n",
        "y = data['mental_health_diagnosed']"
      ],
      "metadata": {
        "id": "siXO_QLSZPxZ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Perform cross-validation**"
      ],
      "metadata": {
        "id": "dFjCysJCE59j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the number of folds for cross-validation\n",
        "num_folds = 5\n",
        "\n",
        "# Create a cross-validation object (KFold)\n",
        "kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)"
      ],
      "metadata": {
        "id": "7tByN42mKxxL"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Baseline Model**"
      ],
      "metadata": {
        "id": "XfIdg6aXZgeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of classifiers\n",
        "classifiers = {\n",
        "    \"Support Vector Machine\": SVC(),\n",
        "    \"Naive Bayes\": GaussianNB(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Logistic Regression\": LogisticRegression()\n",
        "}"
      ],
      "metadata": {
        "id": "dIGDcKKLZfNg"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store evaluation metrics\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for name, clf in classifiers.items():\n",
        "  # Perform cross-validation and get the evaluation scores for each fold\n",
        "  cv_scores = cross_val_score(clf, X, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "  # Calculate the mean and standard deviation of the cross-validation scores\n",
        "  mean_cv_score = np.mean(cv_scores)\n",
        "  std_cv_score = np.std(cv_scores)\n",
        "\n",
        "  results[name] = {\n",
        "      \"MeanCVAccuracy\": mean_cv_score,\n",
        "      \"StdCVAccuracy\": std_cv_score\n",
        "  }\n",
        "\n",
        "\n",
        "# Display the results\n",
        "for name, metrics in results.items():\n",
        "  print(f\"--- {name} ---\")\n",
        "  print(\"Mean Cross-Validation Accuracy:\", round(metrics[\"MeanCVAccuracy\"], 3))\n",
        "  print(\"Standard Deviation of Cross-Validation Accuracy:\", round(metrics[\"StdCVAccuracy\"], 3))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYgKJrw8E69x",
        "outputId": "03237470-5d57-473b-fe55-fcf0fced3344"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Support Vector Machine ---\n",
            "Mean Cross-Validation Accuracy: 0.664\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.01\n",
            "\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Mean Cross-Validation Accuracy: 0.888\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.021\n",
            "\n",
            "\n",
            "--- Decision Tree ---\n",
            "Mean Cross-Validation Accuracy: 0.874\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.011\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Mean Cross-Validation Accuracy: 0.89\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.009\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Apply Imbalance Techniques**"
      ],
      "metadata": {
        "id": "H4BIKT5kd49N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Resampling Methods**"
      ],
      "metadata": {
        "id": "6sVi-joWeCi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **1. Synthetic Minority Over-sampling Technique (SMOTE)**"
      ],
      "metadata": {
        "id": "j0AlXqOZeT69"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "# Create a function for cross-validation with SMOTE\n",
        "def cross_val_with_smote(model, X, y, cv):\n",
        "    cv_scores = []\n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Apply SMOTE to the training data\n",
        "        X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
        "\n",
        "        # Fit the model on the resampled training data\n",
        "        model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        score = model.score(X_test, y_test)\n",
        "        cv_scores.append(score)\n",
        "\n",
        "    return cv_scores"
      ],
      "metadata": {
        "id": "Ljy0qz6yJJpS"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store evaluation metrics\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for name, clf in classifiers.items():\n",
        "  # Perform cross-validation with SMOTE and get the evaluation scores for each fold\n",
        "  cv_scores = cross_val_with_smote(clf, X, y, cv=kfold)\n",
        "\n",
        "  # Calculate the mean and standard deviation of the cross-validation scores\n",
        "  mean_cv_score = np.mean(cv_scores)\n",
        "  std_cv_score = np.std(cv_scores)\n",
        "\n",
        "  results[name] = {\n",
        "      \"MeanCVAccuracy\": mean_cv_score,\n",
        "      \"StdCVAccuracy\": std_cv_score\n",
        "  }\n",
        "\n",
        "\n",
        "# Display the results\n",
        "for name, metrics in results.items():\n",
        "  print(f\"--- {name} ---\")\n",
        "  print(\"Mean Cross-Validation Accuracy:\", round(metrics[\"MeanCVAccuracy\"], 3))\n",
        "  print(\"Standard Deviation of Cross-Validation Accuracy:\", round(metrics[\"StdCVAccuracy\"], 3))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHNHFMEKJjIB",
        "outputId": "594a57b9-d045-4afd-c7cb-1bbe5b5ecdd7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Support Vector Machine ---\n",
            "Mean Cross-Validation Accuracy: 0.753\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.012\n",
            "\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Mean Cross-Validation Accuracy: 0.884\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.018\n",
            "\n",
            "\n",
            "--- Decision Tree ---\n",
            "Mean Cross-Validation Accuracy: 0.858\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.01\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Mean Cross-Validation Accuracy: 0.886\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.012\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **2. Adaptive Synthetic  (ADASYN)**"
      ],
      "metadata": {
        "id": "92WefqQd7IF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and ADASYN\n",
        "adasyn = ADASYN(random_state=42)\n",
        "\n",
        "# Create a function for cross-validation with ADASYN\n",
        "def cross_val_with_adasyn(model, X, y, cv):\n",
        "    cv_scores = []\n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Apply ADASYN to the training data\n",
        "        X_train_resampled, y_train_resampled = adasyn.fit_resample(X_train, y_train)\n",
        "\n",
        "        # Fit the model on the resampled training data\n",
        "        model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        score = model.score(X_test, y_test)\n",
        "        cv_scores.append(score)\n",
        "\n",
        "    return cv_scores"
      ],
      "metadata": {
        "id": "MFyfk_8_KHi5"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store evaluation metrics\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for name, clf in classifiers.items():\n",
        "  # Perform cross-validation with ADASYN and get the evaluation scores for each fold\n",
        "  cv_scores = cross_val_with_adasyn(clf, X, y, cv=kfold)\n",
        "\n",
        "  # Calculate the mean and standard deviation of the cross-validation scores\n",
        "  mean_cv_score = np.mean(cv_scores)\n",
        "  std_cv_score = np.std(cv_scores)\n",
        "\n",
        "  results[name] = {\n",
        "      \"MeanCVAccuracy\": mean_cv_score,\n",
        "      \"StdCVAccuracy\": std_cv_score\n",
        "  }\n",
        "\n",
        "\n",
        "# Display the results\n",
        "for name, metrics in results.items():\n",
        "  print(f\"--- {name} ---\")\n",
        "  print(\"Mean Cross-Validation Accuracy:\", round(metrics[\"MeanCVAccuracy\"], 3))\n",
        "  print(\"Standard Deviation of Cross-Validation Accuracy:\", round(metrics[\"StdCVAccuracy\"], 3))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i9-K0ZqfKd7-",
        "outputId": "689b9b79-ba73-490d-c12b-54066aa65b3a"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Support Vector Machine ---\n",
            "Mean Cross-Validation Accuracy: 0.744\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.01\n",
            "\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Mean Cross-Validation Accuracy: 0.881\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.016\n",
            "\n",
            "\n",
            "--- Decision Tree ---\n",
            "Mean Cross-Validation Accuracy: 0.859\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.012\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Mean Cross-Validation Accuracy: 0.886\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.013\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Tomek Links Under Sampling (TLUS)**"
      ],
      "metadata": {
        "id": "v9AC5YSjAs3q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and TomekLinks\n",
        "tl = TomekLinks()\n",
        "\n",
        "# Create a function for cross-validation with TomekLinks\n",
        "def cross_val_with_tomek(model, X, y, cv):\n",
        "    cv_scores = []\n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Apply TomekLinks to the training data\n",
        "        X_train_resampled, y_train_resampled = tl.fit_resample(X_train, y_train)\n",
        "\n",
        "        # Fit the model on the resampled training data\n",
        "        model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        score = model.score(X_test, y_test)\n",
        "        cv_scores.append(score)\n",
        "\n",
        "    return cv_scores"
      ],
      "metadata": {
        "id": "SDWylTA-K87y"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store evaluation metrics\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for name, clf in classifiers.items():\n",
        "  # Perform cross-validation with TomekLinks and get the evaluation scores for each fold\n",
        "  cv_scores = cross_val_with_tomek(clf, X, y, cv=kfold)\n",
        "\n",
        "  # Calculate the mean and standard deviation of the cross-validation scores\n",
        "  mean_cv_score = np.mean(cv_scores)\n",
        "  std_cv_score = np.std(cv_scores)\n",
        "\n",
        "  results[name] = {\n",
        "      \"MeanCVAccuracy\": mean_cv_score,\n",
        "      \"StdCVAccuracy\": std_cv_score\n",
        "  }\n",
        "\n",
        "\n",
        "# Display the results\n",
        "for name, metrics in results.items():\n",
        "  print(f\"--- {name} ---\")\n",
        "  print(\"Mean Cross-Validation Accuracy:\", round(metrics[\"MeanCVAccuracy\"], 3))\n",
        "  print(\"Standard Deviation of Cross-Validation Accuracy:\", round(metrics[\"StdCVAccuracy\"], 3))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPYAuFi-LUEC",
        "outputId": "4271b394-04bc-4f4f-c117-800fb0a1f2b5"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Support Vector Machine ---\n",
            "Mean Cross-Validation Accuracy: 0.672\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.015\n",
            "\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Mean Cross-Validation Accuracy: 0.884\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.024\n",
            "\n",
            "\n",
            "--- Decision Tree ---\n",
            "Mean Cross-Validation Accuracy: 0.862\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.006\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Mean Cross-Validation Accuracy: 0.889\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.01\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Near Miss Under Sampling (NMUS)**"
      ],
      "metadata": {
        "id": "zf5DJKUjB_uU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the model and NearMiss\n",
        "nm = NearMiss()\n",
        "\n",
        "# Create a function for cross-validation with NearMiss\n",
        "def cross_val_with_nearmiss(model, X, y, cv):\n",
        "    cv_scores = []\n",
        "    for train_idx, test_idx in cv.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        # Apply NearMiss to the training data\n",
        "        X_train_resampled, y_train_resampled = nm.fit_resample(X_train, y_train)\n",
        "\n",
        "        # Fit the model on the resampled training data\n",
        "        model.fit(X_train_resampled, y_train_resampled)\n",
        "\n",
        "        # Evaluate the model on the test data\n",
        "        score = model.score(X_test, y_test)\n",
        "        cv_scores.append(score)\n",
        "\n",
        "    return cv_scores"
      ],
      "metadata": {
        "id": "ki1nOKk4LoAJ"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store evaluation metrics\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for name, clf in classifiers.items():\n",
        "  # Perform cross-validation with Near Miss and get the evaluation scores for each fold\n",
        "  cv_scores = cross_val_with_nearmiss(clf, X, y, cv=kfold)\n",
        "\n",
        "  # Calculate the mean and standard deviation of the cross-validation scores\n",
        "  mean_cv_score = np.mean(cv_scores)\n",
        "  std_cv_score = np.std(cv_scores)\n",
        "\n",
        "  results[name] = {\n",
        "      \"MeanCVAccuracy\": mean_cv_score,\n",
        "      \"StdCVAccuracy\": std_cv_score\n",
        "  }\n",
        "\n",
        "\n",
        "# Display the results\n",
        "for name, metrics in results.items():\n",
        "  print(f\"--- {name} ---\")\n",
        "  print(\"Mean Cross-Validation Accuracy:\", round(metrics[\"MeanCVAccuracy\"], 3))\n",
        "  print(\"Standard Deviation of Cross-Validation Accuracy:\", round(metrics[\"StdCVAccuracy\"], 3))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_FmQFTmL3XC",
        "outputId": "93ae3d61-6c02-48d0-eb8b-6650effa1c53"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Support Vector Machine ---\n",
            "Mean Cross-Validation Accuracy: 0.648\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.014\n",
            "\n",
            "\n",
            "--- Naive Bayes ---\n",
            "Mean Cross-Validation Accuracy: 0.81\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.019\n",
            "\n",
            "\n",
            "--- Decision Tree ---\n",
            "Mean Cross-Validation Accuracy: 0.762\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.008\n",
            "\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Mean Cross-Validation Accuracy: 0.828\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.009\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ensemble Techniques**"
      ],
      "metadata": {
        "id": "sFk-LPWnRhno"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Random Forest**\n",
        "\n",
        "*Ensemble methods like Random Forest are indeed effective in handling class imbalance because they combine multiple weak learners (decision trees) to create a robust and accurate model. The inherent nature of Random Forest helps in addressing the class imbalance problem by reducing the risk of overfitting to the majority class and improving generalization to the minority class.*\n",
        "\n",
        "2. **Boosting Algorithms**\n",
        "\n",
        "*Boosting algorithms are powerful ensemble methods that can handle class imbalance effectively by giving more emphasis to misclassified instances and focusing on difficult-to-classify samples.  One popular boosting algorithm is AdaBoost (Adaptive Boosting).*"
      ],
      "metadata": {
        "id": "xJOgVxXISEJY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Random Forest handles class imbalance effectively because of the following reasons:**\n",
        "\n",
        "1. **Bootstrap Aggregating (Bagging):** Random Forest uses bagging, which means it creates multiple subsets of the training data with replacement. This helps in increasing the representation of the minority class in some of the subsets, making the classifier more robust to imbalanced data.\n",
        "\n",
        "2. **Feature Randomness:** Random Forest selects a random subset of features to split at each node of the decision trees. This randomness further helps in reducing the dominance of the majority class and can improve the overall performance on the minority class.\n",
        "\n",
        "3. **Voting Ensemble:** In the testing phase, the ensemble of decision trees in the Random Forest votes on the final classification. Since each decision tree in the forest may have learned from different subsets of data, it provides a more balanced voting mechanism.\n",
        "\n",
        "4. **Out-of-Bag (OOB) Samples:** Random Forest can also use out-of-bag samples (samples not used during training) to estimate the performance of the model. This helps in getting an unbiased estimate of the model's performance even with imbalanced data.\n",
        "\n",
        "By combining these mechanisms, Random Forest is well-suited for class imbalance problems and can produce reliable and accurate predictions even in the presence of imbalanced classes."
      ],
      "metadata": {
        "id": "ICjLsczTT_va"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AdaBoost handles class imbalance effectively due to the following reasons:**\n",
        "\n",
        "1. **Weighted Voting:** In AdaBoost, each weak learner (typically decision trees) is assigned a weight based on its accuracy in classifying the training data. Misclassified instances are given higher weights, making subsequent weak learners focus more on correcting those errors.\n",
        "\n",
        "2. **Iterative Learning:** AdaBoost iteratively trains weak learners, and at each iteration, it gives more attention to misclassified instances from the previous iteration. This way, difficult-to-classify samples receive more emphasis during the learning process.\n",
        "\n",
        "3. **Ensemble Aggregation:** The final prediction in AdaBoost is made by aggregating the predictions of all weak learners, with more weight given to the ones with better performance on the training data. This ensemble aggregation further helps in handling class imbalance and producing more accurate predictions.\n",
        "\n",
        "4. **Robustness:** By iteratively adapting to the difficult samples, AdaBoost becomes more robust to imbalanced classes over the course of iterations.\n",
        "\n",
        "Overall, AdaBoost can effectively handle class imbalance by focusing on misclassified instances and adapting to the challenges posed by imbalanced data. It often outperforms traditional classifiers when dealing with skewed class distributions."
      ],
      "metadata": {
        "id": "l08TjscGWoCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a list of Ensemble classifiers\n",
        "EnsembleClassifiers = {\n",
        "    \"Adaptive Boosting\": AdaBoostClassifier(random_state=42),\n",
        "    \"Random Forest\": RandomForestClassifier(random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "faGtrj1rVXK0"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store evaluation metrics\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for name, clf in EnsembleClassifiers.items():\n",
        "  # Perform cross-validation and get the evaluation scores for each fold\n",
        "  cv_scores = cross_val_score(clf, X, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "  # Calculate the mean and standard deviation of the cross-validation scores\n",
        "  mean_cv_score = np.mean(cv_scores)\n",
        "  std_cv_score = np.std(cv_scores)\n",
        "\n",
        "  results[name] = {\n",
        "      \"MeanCVAccuracy\": mean_cv_score,\n",
        "      \"StdCVAccuracy\": std_cv_score\n",
        "  }\n",
        "\n",
        "\n",
        "# Display the results\n",
        "for name, metrics in results.items():\n",
        "  print(f\"--- {name} ---\")\n",
        "  print(\"Mean Cross-Validation Accuracy:\", round(metrics[\"MeanCVAccuracy\"], 3))\n",
        "  print(\"Standard Deviation of Cross-Validation Accuracy:\", round(metrics[\"StdCVAccuracy\"], 3))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CPrMji8KMKz8",
        "outputId": "cbda14bd-8d27-4187-f8b9-379a44b2b45a"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Adaptive Boosting ---\n",
            "Mean Cross-Validation Accuracy: 0.915\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.011\n",
            "\n",
            "\n",
            "--- Random Forest ---\n",
            "Mean Cross-Validation Accuracy: 0.919\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.009\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Algorithm-specific Methods**"
      ],
      "metadata": {
        "id": "gj__YJteYJiS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost (Extreme Gradient Boosting) is another powerful boosting algorithm that has specific parameters and techniques to handle class imbalance effectively. XGBoost is an enhanced version of Gradient Boosting that leverages a variety of regularization techniques and can handle class imbalance naturally.\n",
        "\n",
        "We used XGBoost in algorithm specific methods because it had specific parameters or techniques to handle class imbalance.\n",
        "\n"
      ],
      "metadata": {
        "id": "taCJ20ADYOAZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**XGBoost can handle class imbalance effectively due to the following reasons:**\n",
        "\n",
        "**scale_pos_weight:** The scale_pos_weight parameter helps in handling class imbalance by assigning higher weights to the minority class during the boosting process. This parameter helps in balancing the effect of class distribution and prevents the model from being biased towards the majority class.\n",
        "\n",
        "**Regularization:** XGBoost uses L1 and L2 regularization to prevent overfitting, which can be beneficial when dealing with imbalanced data as it reduces the risk of overfitting to the majority class.\n",
        "\n",
        "**Gradient-based Optimization:** XGBoost employs gradient-based optimization techniques, which allows it to prioritize difficult-to-classify samples during the boosting process, thus making it more robust to imbalanced classes.\n",
        "\n",
        "Overall, XGBoost is an excellent choice for handling class imbalance, as it provides built-in mechanisms to deal with skewed class distributions while delivering high performance and accurate predictions."
      ],
      "metadata": {
        "id": "2rAl3QFQaAZa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and set up the XGBoost classifier\n",
        "# You can use scale_pos_weight to handle class imbalance by assigning higher weights to the minority class\n",
        "# The scale_pos_weight should be set as the ratio of the number of negative (majority) class samples to positive (minority) class samples\n",
        "\n",
        "scale_pos_weight = 592 / 1159\n",
        "\n",
        "# Create and set up the LightGBM classifier\n",
        "# You can use the is_unbalance parameter to handle class imbalance by automatically setting the positive (minority) class weight\n",
        "\n",
        "\n",
        "AlgorithmSpecific = {\n",
        "    \"XGBoost\": XGBClassifier(scale_pos_weight=scale_pos_weight, random_state=42),\n",
        "    \"LightGBM\": lgb.LGBMClassifier(is_unbalance=True, random_state=42)\n",
        "}"
      ],
      "metadata": {
        "id": "YT58yyW_YzTJ"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary to store evaluation metrics\n",
        "results = {}\n",
        "\n",
        "# Loop through each classifier\n",
        "for name, clf in AlgorithmSpecific.items():\n",
        "  # Perform cross-validation and get the evaluation scores for each fold\n",
        "  cv_scores = cross_val_score(clf, X, y, cv=kfold, scoring='accuracy')\n",
        "\n",
        "  # Calculate the mean and standard deviation of the cross-validation scores\n",
        "  mean_cv_score = np.mean(cv_scores)\n",
        "  std_cv_score = np.std(cv_scores)\n",
        "\n",
        "  results[name] = {\n",
        "      \"MeanCVAccuracy\": mean_cv_score,\n",
        "      \"StdCVAccuracy\": std_cv_score\n",
        "  }\n",
        "\n",
        "\n",
        "# Display the results\n",
        "for name, metrics in results.items():\n",
        "  print(f\"--- {name} ---\")\n",
        "  print(\"Mean Cross-Validation Accuracy:\", round(metrics[\"MeanCVAccuracy\"], 3))\n",
        "  print(\"Standard Deviation of Cross-Validation Accuracy:\", round(metrics[\"StdCVAccuracy\"], 3))\n",
        "  print(\"\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRTzN_m5N6Ga",
        "outputId": "71c77a79-605a-4cec-c9ea-ebe2a7bcfa6a"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- XGBoost ---\n",
            "Mean Cross-Validation Accuracy: 0.91\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.011\n",
            "\n",
            "\n",
            "--- LightGBM ---\n",
            "Mean Cross-Validation Accuracy: 0.918\n",
            "Standard Deviation of Cross-Validation Accuracy: 0.009\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}